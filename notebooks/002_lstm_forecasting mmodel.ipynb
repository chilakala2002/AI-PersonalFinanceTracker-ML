{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T15:29:43.881032Z",
     "start_time": "2025-03-21T15:29:43.777607Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the correct dataset for forecasting\n",
    "df = pd.read_csv(\"../data/processed_data/forecasting_data.csv\")\n",
    "\n",
    "# Ensure data is sorted by date if not ready\n",
    "df = df.sort_values(by=[\"year\", \"month\", \"day\"])\n",
    "\n",
    "# Create a new 'date' column for time-series indexing\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "\n",
    "# Set date as index and drop redundant columns\n",
    "df.set_index(\"date\", inplace=True)\n",
    "df.drop(columns=[\"year\", \"month\", \"day\"], inplace=True)\n",
    "\n",
    "print(\" Data Loaded from forecasting_data.csv!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Loaded from forecasting_data.csv!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:29:49.907675Z",
     "start_time": "2025-03-21T15:29:43.890832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequences(data, target_col, window_size=6):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data.iloc[i:i+window_size].values)  # Features: Past 6 months\n",
    "        y.append(data.iloc[i+window_size][target_col])  # Target: Next month’s spending\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define look-back window size (6 months)\n",
    "window_size = 6\n",
    "\n",
    "# Generate sequences from forecasting_data.csv\n",
    "X, y = create_sequences(df, target_col=\"amount_log\", window_size=window_size)\n",
    "\n",
    "print(f\" Created {X.shape[0]} training sequences!\")"
   ],
   "id": "e0899fed5d49278a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created 149994 training sequences!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:29:49.931931Z",
     "start_time": "2025-03-21T15:29:49.918380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(f\" Training Set: {X_train.shape[0]} samples\")\n",
    "print(f\" Testing Set: {X_test.shape[0]} samples\")"
   ],
   "id": "cb36f86b275c1437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Set: 119995 samples\n",
      " Testing Set: 29999 samples\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:28.952952Z",
     "start_time": "2025-03-21T15:29:49.954831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(window_size, X_train.shape[2])),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  # Output layer predicts log-transformed spending\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\" LSTM Model Training Complete!\")"
   ],
   "id": "258d7cf93db3f4a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiajaychilakala/AI-PersonalFinanceTracker-ML/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 2ms/step - loss: 0.1437 - val_loss: 0.1181\n",
      "Epoch 2/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1182 - val_loss: 0.1239\n",
      "Epoch 3/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1188 - val_loss: 0.1148\n",
      "Epoch 4/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1175 - val_loss: 0.1153\n",
      "Epoch 5/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1173 - val_loss: 0.1152\n",
      "Epoch 6/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1170 - val_loss: 0.1154\n",
      "Epoch 7/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1171 - val_loss: 0.1160\n",
      "Epoch 8/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1163 - val_loss: 0.1170\n",
      "Epoch 9/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1165 - val_loss: 0.1150\n",
      "Epoch 10/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1163 - val_loss: 0.1152\n",
      "Epoch 11/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1169 - val_loss: 0.1151\n",
      "Epoch 12/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1169 - val_loss: 0.1180\n",
      "Epoch 13/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1158 - val_loss: 0.1149\n",
      "Epoch 14/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1173 - val_loss: 0.1151\n",
      "Epoch 15/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1164 - val_loss: 0.1155\n",
      "Epoch 16/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1169 - val_loss: 0.1149\n",
      "Epoch 17/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1167 - val_loss: 0.1149\n",
      "Epoch 18/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1171 - val_loss: 0.1160\n",
      "Epoch 19/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1165 - val_loss: 0.1155\n",
      "Epoch 20/20\n",
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 2ms/step - loss: 0.1170 - val_loss: 0.1155\n",
      " LSTM Model Training Complete!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:48.214896Z",
     "start_time": "2025-03-21T15:32:44.881367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lstm_model.predict(X_train)\n",
    "y_test_pred = lstm_model.predict(X_test)\n",
    "\n",
    "# Compute MAE & RMSE\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"\\n LSTM Model Performance:\")\n",
    "print(f\"Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")"
   ],
   "id": "c48cb6c154fc26c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 592us/step\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 575us/step\n",
      "\n",
      " LSTM Model Performance:\n",
      "Train MAE: 0.2706, Test MAE: 0.2689\n",
      "Train RMSE: 0.3417, Test RMSE: 0.3398\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:48.245067Z",
     "start_time": "2025-03-21T15:32:48.218377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_future_spending(recent_data, model, window_size=6):\n",
    "    \"\"\"\n",
    "    Predict future spending using the trained LSTM model.\n",
    "    `recent_data`: Last 6 months of spending.\n",
    "    \"\"\"\n",
    "    # Reshape for LSTM input\n",
    "    input_seq = np.expand_dims(recent_data.values, axis=0)\n",
    "\n",
    "    # Predict log spending amount\n",
    "    predicted_log_spending = model.predict(input_seq)[0][0]\n",
    "\n",
    "    # Convert back to original scale\n",
    "    predicted_spending = np.exp(predicted_log_spending)\n",
    "\n",
    "    return predicted_spending\n",
    "\n",
    "# Predict next month's spending using last 6 months' data\n",
    "last_6_months = df.iloc[-6:]\n",
    "predicted_value = predict_future_spending(last_6_months, lstm_model)\n",
    "\n",
    "print(f\"\\n Predicted Spending for Next Month: ${predicted_value:.2f}\")"
   ],
   "id": "6fb95d4992ae3aa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\n",
      " Predicted Spending for Next Month: $4.05\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:48.265758Z",
     "start_time": "2025-03-21T15:32:48.258235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reverse log transformation to get actual spending values\n",
    "df[\"actual_spending\"] = np.exp(df[\"amount_log\"])\n",
    "\n",
    "# Display statistics\n",
    "df[\"actual_spending\"].describe()"
   ],
   "id": "eea50988af352bd8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    150000.000000\n",
       "mean          4.348479\n",
       "std           1.240324\n",
       "min           1.000000\n",
       "25%           3.542389\n",
       "50%           4.571221\n",
       "75%           5.276666\n",
       "max           7.873368\n",
       "Name: actual_spending, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:48.313889Z",
     "start_time": "2025-03-21T15:32:48.279023Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "1347bb79a59207c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         weekday_num       mcc_freq  merchant_category_encoded      is_refund  \\\n",
       "count  150000.000000  150000.000000              150000.000000  150000.000000   \n",
       "mean        3.001693       0.061277                  58.476447       0.049540   \n",
       "std         1.998532       0.040969                  28.844909       0.216993   \n",
       "min         0.000000       0.000024                   0.000000       0.000000   \n",
       "25%         1.000000       0.035727                  34.000000       0.000000   \n",
       "50%         3.000000       0.050664                  61.000000       0.000000   \n",
       "75%         5.000000       0.107074                  87.000000       0.000000   \n",
       "max         6.000000       0.119690                 107.000000       1.000000   \n",
       "\n",
       "       per_capita_income  yearly_income    total_debt     amount_log  \\\n",
       "count       1.500000e+05   1.500000e+05  1.500000e+05  150000.000000   \n",
       "mean       -1.114131e-16   2.782959e-17 -2.946384e-17       1.419006   \n",
       "std         1.000003e+00   1.000003e+00  1.000003e+00       0.340785   \n",
       "min        -2.572172e+00  -2.438146e+00 -1.207768e+00       0.000000   \n",
       "25%        -7.388124e-01  -6.957376e-01 -8.573371e-01       1.264801   \n",
       "50%        -2.265618e-01  -2.122563e-01 -1.059912e-01       1.519780   \n",
       "75%         5.519426e-01   5.428254e-01  6.033125e-01       1.663294   \n",
       "max         2.219384e+00   2.208510e+00  2.772522e+00       2.063486   \n",
       "\n",
       "       actual_spending  \n",
       "count    150000.000000  \n",
       "mean          4.348479  \n",
       "std           1.240324  \n",
       "min           1.000000  \n",
       "25%           3.542389  \n",
       "50%           4.571221  \n",
       "75%           5.276666  \n",
       "max           7.873368  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_num</th>\n",
       "      <th>mcc_freq</th>\n",
       "      <th>merchant_category_encoded</th>\n",
       "      <th>is_refund</th>\n",
       "      <th>per_capita_income</th>\n",
       "      <th>yearly_income</th>\n",
       "      <th>total_debt</th>\n",
       "      <th>amount_log</th>\n",
       "      <th>actual_spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>1.500000e+05</td>\n",
       "      <td>1.500000e+05</td>\n",
       "      <td>1.500000e+05</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.001693</td>\n",
       "      <td>0.061277</td>\n",
       "      <td>58.476447</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>-1.114131e-16</td>\n",
       "      <td>2.782959e-17</td>\n",
       "      <td>-2.946384e-17</td>\n",
       "      <td>1.419006</td>\n",
       "      <td>4.348479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.998532</td>\n",
       "      <td>0.040969</td>\n",
       "      <td>28.844909</td>\n",
       "      <td>0.216993</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.340785</td>\n",
       "      <td>1.240324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.572172e+00</td>\n",
       "      <td>-2.438146e+00</td>\n",
       "      <td>-1.207768e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.388124e-01</td>\n",
       "      <td>-6.957376e-01</td>\n",
       "      <td>-8.573371e-01</td>\n",
       "      <td>1.264801</td>\n",
       "      <td>3.542389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.265618e-01</td>\n",
       "      <td>-2.122563e-01</td>\n",
       "      <td>-1.059912e-01</td>\n",
       "      <td>1.519780</td>\n",
       "      <td>4.571221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.107074</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.519426e-01</td>\n",
       "      <td>5.428254e-01</td>\n",
       "      <td>6.033125e-01</td>\n",
       "      <td>1.663294</td>\n",
       "      <td>5.276666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.119690</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.219384e+00</td>\n",
       "      <td>2.208510e+00</td>\n",
       "      <td>2.772522e+00</td>\n",
       "      <td>2.063486</td>\n",
       "      <td>7.873368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:32:50.777103Z",
     "start_time": "2025-03-21T15:32:48.334194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict on training data and compare\n",
    "train_preds = lstm_model.predict(X_train)\n",
    "train_actuals = np.exp(y_train)  # Convert from log-scale\n",
    "\n",
    "# Compare the mean predicted vs actual spending\n",
    "print(f\"Mean Predicted Spending: ${np.mean(np.exp(train_preds)):.2f}\")\n",
    "print(f\"Mean Actual Spending: ${np.mean(train_actuals):.2f}\")"
   ],
   "id": "9fa1c4a352fbdba3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 579us/step\n",
      "Mean Predicted Spending: $4.05\n",
      "Mean Actual Spending: $4.35\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:35:40.354659Z",
     "start_time": "2025-03-21T15:32:50.797375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define an improved LSTM Model\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dropout(0.2),  # Reduce overfitting\n",
    "        LSTM(units=64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dense(units=1)  # Final output\n",
    "    ])\n",
    "\n",
    "    # Compile the model with optimized learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train the new model\n",
    "lstm_model = build_lstm_model()\n",
    "history = lstm_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)"
   ],
   "id": "f09156ab1cf1b045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiajaychilakala/AI-PersonalFinanceTracker-ML/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 6ms/step - loss: 0.1543 - mae: 0.2968 - val_loss: 0.1229 - val_mae: 0.2916\n",
      "Epoch 2/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1204 - mae: 0.2700 - val_loss: 0.1214 - val_mae: 0.2884\n",
      "Epoch 3/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - loss: 0.1186 - mae: 0.2679 - val_loss: 0.1188 - val_mae: 0.2819\n",
      "Epoch 4/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1177 - mae: 0.2670 - val_loss: 0.1227 - val_mae: 0.2913\n",
      "Epoch 5/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1180 - mae: 0.2670 - val_loss: 0.1219 - val_mae: 0.2896\n",
      "Epoch 6/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1169 - mae: 0.2662 - val_loss: 0.1248 - val_mae: 0.2958\n",
      "Epoch 7/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1173 - mae: 0.2668 - val_loss: 0.1252 - val_mae: 0.2967\n",
      "Epoch 8/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1168 - mae: 0.2659 - val_loss: 0.1235 - val_mae: 0.2931\n",
      "Epoch 9/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1165 - mae: 0.2655 - val_loss: 0.1203 - val_mae: 0.2857\n",
      "Epoch 10/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1162 - mae: 0.2653 - val_loss: 0.1221 - val_mae: 0.2899\n",
      "Epoch 11/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1166 - mae: 0.2656 - val_loss: 0.1236 - val_mae: 0.2932\n",
      "Epoch 12/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1170 - mae: 0.2662 - val_loss: 0.1188 - val_mae: 0.2818\n",
      "Epoch 13/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1165 - mae: 0.2656 - val_loss: 0.1198 - val_mae: 0.2846\n",
      "Epoch 14/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1167 - mae: 0.2657 - val_loss: 0.1228 - val_mae: 0.2916\n",
      "Epoch 15/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - loss: 0.1164 - mae: 0.2655 - val_loss: 0.1170 - val_mae: 0.2767\n",
      "Epoch 16/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1161 - mae: 0.2652 - val_loss: 0.1167 - val_mae: 0.2755\n",
      "Epoch 17/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - loss: 0.1161 - mae: 0.2649 - val_loss: 0.1181 - val_mae: 0.2800\n",
      "Epoch 18/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1167 - mae: 0.2656 - val_loss: 0.1157 - val_mae: 0.2717\n",
      "Epoch 19/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1151 - mae: 0.2641 - val_loss: 0.1159 - val_mae: 0.2726\n",
      "Epoch 20/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1165 - mae: 0.2659 - val_loss: 0.1208 - val_mae: 0.2870\n",
      "Epoch 21/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1163 - mae: 0.2658 - val_loss: 0.1180 - val_mae: 0.2797\n",
      "Epoch 22/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1160 - mae: 0.2652 - val_loss: 0.1153 - val_mae: 0.2697\n",
      "Epoch 23/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1154 - mae: 0.2637 - val_loss: 0.1165 - val_mae: 0.2748\n",
      "Epoch 24/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1181 - mae: 0.2676 - val_loss: 0.1156 - val_mae: 0.2713\n",
      "Epoch 25/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1166 - mae: 0.2658 - val_loss: 0.1162 - val_mae: 0.2737\n",
      "Epoch 26/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1174 - mae: 0.2668 - val_loss: 0.1170 - val_mae: 0.2767\n",
      "Epoch 27/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m361s\u001B[0m 193ms/step - loss: 0.1167 - mae: 0.2660 - val_loss: 0.1165 - val_mae: 0.2750\n",
      "Epoch 28/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m245s\u001B[0m 131ms/step - loss: 0.1167 - mae: 0.2661 - val_loss: 0.1157 - val_mae: 0.2719\n",
      "Epoch 29/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m578s\u001B[0m 309ms/step - loss: 0.1167 - mae: 0.2661 - val_loss: 0.1153 - val_mae: 0.2698\n",
      "Epoch 30/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m137s\u001B[0m 73ms/step - loss: 0.1174 - mae: 0.2669 - val_loss: 0.1164 - val_mae: 0.2746\n",
      "Epoch 31/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m169s\u001B[0m 90ms/step - loss: 0.1157 - mae: 0.2649 - val_loss: 0.1178 - val_mae: 0.2791\n",
      "Epoch 32/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m313s\u001B[0m 167ms/step - loss: 0.1159 - mae: 0.2650 - val_loss: 0.1176 - val_mae: 0.2786\n",
      "Epoch 33/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1177 - mae: 0.2667 - val_loss: 0.1151 - val_mae: 0.2687\n",
      "Epoch 34/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1168 - mae: 0.2661 - val_loss: 0.1154 - val_mae: 0.2704\n",
      "Epoch 35/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - loss: 0.1166 - mae: 0.2656 - val_loss: 0.1147 - val_mae: 0.2649\n",
      "Epoch 36/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - loss: 0.1178 - mae: 0.2671 - val_loss: 0.1147 - val_mae: 0.2648\n",
      "Epoch 37/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1172 - mae: 0.2668 - val_loss: 0.1149 - val_mae: 0.2671\n",
      "Epoch 38/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1163 - mae: 0.2657 - val_loss: 0.1150 - val_mae: 0.2678\n",
      "Epoch 39/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1166 - mae: 0.2657 - val_loss: 0.1147 - val_mae: 0.2628\n",
      "Epoch 40/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - loss: 0.1167 - mae: 0.2658 - val_loss: 0.1157 - val_mae: 0.2714\n",
      "Epoch 41/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1170 - mae: 0.2662 - val_loss: 0.1147 - val_mae: 0.2637\n",
      "Epoch 42/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1166 - mae: 0.2660 - val_loss: 0.1150 - val_mae: 0.2681\n",
      "Epoch 43/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - loss: 0.1165 - mae: 0.2660 - val_loss: 0.1150 - val_mae: 0.2679\n",
      "Epoch 44/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - loss: 0.1164 - mae: 0.2655 - val_loss: 0.1148 - val_mae: 0.2653\n",
      "Epoch 45/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 38ms/step - loss: 0.1183 - mae: 0.2673 - val_loss: 0.1153 - val_mae: 0.2697\n",
      "Epoch 46/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m488s\u001B[0m 260ms/step - loss: 0.1163 - mae: 0.2652 - val_loss: 0.1148 - val_mae: 0.2648\n",
      "Epoch 47/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 22ms/step - loss: 0.1178 - mae: 0.2668 - val_loss: 0.1149 - val_mae: 0.2662\n",
      "Epoch 48/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m276s\u001B[0m 147ms/step - loss: 0.1162 - mae: 0.2649 - val_loss: 0.1151 - val_mae: 0.2684\n",
      "Epoch 49/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m613s\u001B[0m 327ms/step - loss: 0.1170 - mae: 0.2665 - val_loss: 0.1148 - val_mae: 0.2652\n",
      "Epoch 50/50\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 8ms/step - loss: 0.1175 - mae: 0.2668 - val_loss: 0.1148 - val_mae: 0.2656\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T17:24:45.552589Z",
     "start_time": "2025-03-21T17:24:39.700338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict train and test data\n",
    "train_preds = lstm_model.predict(X_train)\n",
    "test_preds = lstm_model.predict(X_test)\n",
    "\n",
    "# Convert predictions from log scale back to actual spending\n",
    "train_actuals = np.exp(y_train)\n",
    "test_actuals = np.exp(y_test)\n",
    "train_preds = np.exp(train_preds)\n",
    "test_preds = np.exp(test_preds)\n",
    "\n",
    "# Compute performance metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "train_mae = mean_absolute_error(train_actuals, train_preds)\n",
    "test_mae = mean_absolute_error(test_actuals, test_preds)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(train_actuals, train_preds))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_actuals, test_preds))\n",
    "\n",
    "print(f\"LSTM Model Performance After Tuning:\")\n",
    "print(f\"Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")"
   ],
   "id": "111a7d355157c0fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3750/3750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 1ms/step\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1ms/step\n",
      "LSTM Model Performance After Tuning:\n",
      "Train MAE: 1.0634, Test MAE: 1.0543\n",
      "Train RMSE: 1.2676, Test RMSE: 1.2591\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T17:24:45.568214Z",
     "start_time": "2025-03-21T17:24:45.557912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained LSTM forecasting model\n",
    "lstm_model.save(\"../data/models/lstm_forecasting_model.h5\")  # Save in .h5 format\n",
    "print(\"LSTM Forecasting Model Saved Successfully!\")"
   ],
   "id": "d93a975b59741cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Forecasting Model Saved Successfully!\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:38:17.363356Z",
     "start_time": "2025-03-25T15:38:17.360530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "f1331483aea6fc6c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:38:21.318719Z",
     "start_time": "2025-03-25T15:38:21.112950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained LSTM forecasting model with explicit loss function\n",
    "lstm_model = load_model(\n",
    "    \"../data/models/lstm_forecasting_model.h5\",\n",
    "    custom_objects={\"mse\": MeanSquaredError()}\n",
    ")\n",
    "print(\" LSTM Forecasting Model Loaded Successfully!\")\n",
    "\n",
    "# Load the classification model for merchant category prediction (LSTM)\n",
    "lstm_classification_model = load_model(\"../data/models/lstm_model.h5\")\n",
    "print(\" LSTM Classification Model Loaded Successfully!\")\n",
    "\n",
    "# Load tokenizer for text processing\n",
    "with open(\"../data/models/tokenizer.pkl\", \"rb\") as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "print(\" Tokenizer Loaded Successfully!\")\n",
    "\n",
    "# Load label encoder (for merchant category classification)\n",
    "with open(\"../data/models/label_encoder.pkl\", \"rb\") as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "print(\" Label Encoder Loaded Successfully!\")\n",
    "\n",
    "# Load processed dataset (ensure it contains the required features)\n",
    "df = pd.read_csv(\"../data/processed_data/forecasting_data.csv\")\n",
    "\n",
    "# Define Features Used in Forecasting Model (Fixed to 8 Features)\n",
    "feature_columns = [\n",
    "    \"year\", \"month\", \"day\", \"weekday_num\", \"mcc_freq\",\n",
    "    \"merchant_category_encoded\", \"yearly_income\", \"total_debt\"\n",
    "]\n",
    "\n",
    "print(\" All Required Data & Models Loaded Successfully!\")"
   ],
   "id": "17f57274ef955373",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Forecasting Model Loaded Successfully!\n",
      " LSTM Classification Model Loaded Successfully!\n",
      " Tokenizer Loaded Successfully!\n",
      " Label Encoder Loaded Successfully!\n",
      " All Required Data & Models Loaded Successfully!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:38:28.394413Z",
     "start_time": "2025-03-25T15:38:28.391241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_predicted_category(description):\n",
    "    \"\"\"\n",
    "    Predicts the merchant category based on transaction description using the classification model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Processing Description: '{description}'\")\n",
    "\n",
    "    # Convert text to numerical sequence\n",
    "    desc_sequence = tokenizer.texts_to_sequences([description])\n",
    "    desc_padded = pad_sequences(desc_sequence, maxlen=20)  # Ensure fixed input size\n",
    "\n",
    "    # Predict category using LSTM classification model\n",
    "    predicted_category = lstm_classification_model.predict(desc_padded)\n",
    "    predicted_category = np.argmax(predicted_category, axis=1)[0]  # Get highest probability class\n",
    "\n",
    "    # Convert category index to actual label\n",
    "    category_label = label_encoder.inverse_transform([predicted_category])[0]\n",
    "\n",
    "    print(f\" Predicted Category: {category_label} (Encoded: {predicted_category})\")\n",
    "    return predicted_category"
   ],
   "id": "74036a7e204cb5f5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:42:04.482132Z",
     "start_time": "2025-03-25T15:42:04.475649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "def predict_future_spending(year, month, day, amount, description, lstm_model, df, feature_columns, user_profile, window_size=6):\n",
    "    \"\"\"\n",
    "    Predicts future spending using the trained LSTM model based on user input and historical patterns.\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning Future Spending Prediction...\")\n",
    "\n",
    "    # Step 1: Predict merchant category from description\n",
    "    predicted_category = get_predicted_category(description)\n",
    "\n",
    "    # Step 2: Extract weekday and encode spending amount\n",
    "    weekday_num = datetime.date(year, month, day).weekday()\n",
    "    amount_log = np.log(amount)\n",
    "\n",
    "    # Step 3: Handle income & debt\n",
    "    if \"yearly_income\" not in user_profile or \"total_debt\" not in user_profile:\n",
    "        print(\"First-time user: Income & debt stored for future use.\")\n",
    "        user_profile[\"yearly_income\"] = float(input(\"Enter your yearly income: \"))\n",
    "        user_profile[\"total_debt\"] = float(input(\"Enter your total debt: \"))\n",
    "    else:\n",
    "        print(\"Returning user detected, fetching stored income & debt.\")\n",
    "\n",
    "    yearly_income = user_profile[\"yearly_income\"]\n",
    "    total_debt = user_profile[\"total_debt\"]\n",
    "\n",
    "    # Step 4: Filter last N transactions for that merchant category\n",
    "    category_history = df[df[\"merchant_category_encoded\"] == predicted_category].copy()\n",
    "    category_history.sort_values(by=[\"year\", \"month\", \"day\"], inplace=True)\n",
    "\n",
    "    recent_history = category_history.tail(window_size)\n",
    "\n",
    "    # Step 5: Pad if fewer than window_size transactions exist\n",
    "    if len(recent_history) < window_size:\n",
    "        print(\"Insufficient history. Padding with median values.\")\n",
    "        pad_rows = pd.DataFrame([df[feature_columns].median()] * (window_size - len(recent_history)))\n",
    "        recent_history = pd.concat([pad_rows, recent_history], ignore_index=True)\n",
    "\n",
    "    # Step 6: Replace static features in the sequence with current user values\n",
    "    recent_history[\"year\"] = year\n",
    "    recent_history[\"month\"] = month\n",
    "    recent_history[\"day\"] = day\n",
    "    recent_history[\"weekday_num\"] = weekday_num\n",
    "    recent_history[\"merchant_category_encoded\"] = predicted_category\n",
    "    recent_history.loc[:, \"yearly_income\"] = yearly_income\n",
    "    recent_history.loc[:, \"total_debt\"] = total_debt\n",
    "\n",
    "    # Step 7: Prepare input sequence\n",
    "    input_seq = recent_history[feature_columns].values.reshape(1, window_size, len(feature_columns))\n",
    "    print(f\"Input Sequence Shape: {input_seq.shape}\")\n",
    "\n",
    "    # Step 8: Predict and return output\n",
    "    predicted_log_spending = lstm_model.predict(input_seq)[0][0]\n",
    "    predicted_spending = np.exp(predicted_log_spending)\n",
    "\n",
    "    print(f\" Predicted Spending (log-scale): {predicted_log_spending}\")\n",
    "    print(f\" Final Predicted Spending: ${predicted_spending:.2f}\")\n",
    "\n",
    "    return predicted_spending"
   ],
   "id": "e2e7e1529a571e03",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:42:35.759291Z",
     "start_time": "2025-03-25T15:42:24.604992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_profile = {}\n",
    "\n",
    "# First-time user input\n",
    "year = 2025\n",
    "month = 3\n",
    "day = 21\n",
    "amount = 15.00\n",
    "description = \"Uber ride home\"\n",
    "\n",
    "predicted_value = predict_future_spending(\n",
    "    year, month, day, amount, description,\n",
    "    lstm_model, df, feature_columns, user_profile\n",
    ")\n",
    "\n",
    "print(f\"\\n Predicted Spending for Next Month: ${predicted_value:.2f}\")"
   ],
   "id": "7179190984b1a04e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Future Spending Prediction...\n",
      "\n",
      " Processing Description: 'Uber ride home'\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      " Predicted Category: 10 (Encoded: 10)\n",
      "First-time user: Income & debt stored for future use.\n",
      "Input Sequence Shape: (1, 6, 8)\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      " Predicted Spending (log-scale): 1.4330188035964966\n",
      " Final Predicted Spending: $4.19\n",
      "\n",
      " Predicted Spending for Next Month: $4.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"year\"] = year\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"month\"] = month\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"day\"] = day\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"weekday_num\"] = weekday_num\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"merchant_category_encoded\"] = predicted_category\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:49:29.941609Z",
     "start_time": "2025-03-25T15:49:29.878940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize user profile with stored income & debt\n",
    "user_profile = {\n",
    "    \"yearly_income\": 75000,  # Example income\n",
    "    \"total_debt\": 5000       # Example debt\n",
    "}\n",
    "\n",
    "# Define test input for a returning user\n",
    "year = 2025\n",
    "month = 4\n",
    "day = 10\n",
    "amount = 30.75  # Amount in dollars\n",
    "description = \"Starbucks coffee\"\n",
    "\n",
    "# Run the prediction function\n",
    "predicted_value = predict_future_spending(year, month, day, amount, description, lstm_model, df, feature_columns, user_profile)\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"\\n Predicted Spending for Next Month: ${predicted_value:.2f}\")"
   ],
   "id": "4498f87eced42881",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Future Spending Prediction...\n",
      "\n",
      " Processing Description: 'Starbucks coffee'\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      " Predicted Category: 10 (Encoded: 10)\n",
      "Returning user detected, fetching stored income & debt.\n",
      "Input Sequence Shape: (1, 6, 8)\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step\n",
      " Predicted Spending (log-scale): 1.4225270748138428\n",
      " Final Predicted Spending: $4.15\n",
      "\n",
      " Predicted Spending for Next Month: $4.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"year\"] = year\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"month\"] = month\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"day\"] = day\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"weekday_num\"] = weekday_num\n",
      "/var/folders/31/0snxt4w95wn189bxy4bgfd000000gn/T/ipykernel_76130/3628956853.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_history[\"merchant_category_encoded\"] = predicted_category\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
